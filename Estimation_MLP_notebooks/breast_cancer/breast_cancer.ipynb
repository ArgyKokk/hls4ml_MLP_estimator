{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 01:49:28.227540: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-19 01:49:28.323293: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from callbacks import all_callbacks\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "import tensorflow.compat.v1 as tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(683, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./breast-cancer-wisconsin.csv', sep = ';')\n",
    "print (np.shape(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2\n",
      "1      2\n",
      "2      2\n",
      "3      2\n",
      "4      2\n",
      "      ..\n",
      "678    2\n",
      "679    2\n",
      "680    3\n",
      "681    3\n",
      "682    3\n",
      "Name: Y, Length: 683, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('Y', axis = 1).values\n",
    "y = df.Y\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "Y_reshaped = [[label] for label in y]\n",
    "Y_encoded = encoder.fit_transform(Y_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y_encoded,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range=(0,0.9))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (478, 10)\n",
      "Shape of X_test:  (205, 10)\n",
      "Shape of y_train:  (478, 2)\n",
      "Shape of y_test (205, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train: \",X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \",Y_train.shape)\n",
    "print(\"Shape of y_test\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# clf = MLPClassifier(beta_1=0.004044058262057914, beta_2=0.2692099545241596,\n",
    "#               epsilon=0.4100816459563625, hidden_layer_sizes=3, max_iter=150,\n",
    "#               momentum=0.8221177331942455, nesterovs_momentum=False,\n",
    "#               solver='lbfgs', validation_fraction=0.511318982546456,alpha=0.0001).fit(X_train, Y_train)\n",
    "# clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(clf,'./Seeds_argykokk_clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store the weights, biases of the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu,quantized_po2\n",
    "import tensorflow.compat.v1 as tf1\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from qkeras.utils import model_save_quantized_weights\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Sequential()\n",
    "\n",
    "# model.add(QDense(5, input_shape=(10,), name='fc1', kernel_quantizer=quantized_bits(8,0,alpha=1,use_stochastic_rounding=True),bias_quantizer=quantized_bits(8,0,alpha=1),\n",
    "#                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)   ))\n",
    "# model.add(QActivation(activation=quantized_relu(8,0,use_stochastic_rounding=False), name='relu1'))\n",
    "# model.add(QDense(3, name='fc2',\n",
    "#                 kernel_quantizer=quantized_bits(8,0,alpha=2,use_stochastic_rounding=True), bias_quantizer=quantized_bits(8,0,alpha=1),\n",
    "#                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "# model.add(QActivation(activation=quantized_relu(8,0,use_stochastic_rounding=False), name='relu2'))\n",
    "# model.add(QDense(2, name='output',\n",
    "#                 kernel_quantizer=quantized_bits(8,0,alpha=2,use_stochastic_rounding=True), bias_quantizer=quantized_bits(8,0,alpha=1),\n",
    "#                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "# model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  1/382 [..............................] - ETA: 5:30 - loss: 0.1995 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0033s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0033s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/382 [===========================>..] - ETA: 0s - loss: 0.5588 - accuracy: 0.8164\n",
      "***callbacks***\n",
      "saving losses to BC_classification_prune/losses.log\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.40112, saving model to BC_classification_prune/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.40112, saving model to BC_classification_prune/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 1: saving model to BC_classification_prune/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 1: saving model to BC_classification_prune/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "382/382 [==============================] - 3s 5ms/step - loss: 0.5509 - accuracy: 0.8194 - val_loss: 0.4011 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "366/382 [===========================>..] - ETA: 0s - loss: 0.2805 - accuracy: 0.9454\n",
      "***callbacks***\n",
      "saving losses to BC_classification_prune/losses.log\n",
      "\n",
      "Epoch 2: val_loss improved from 0.40112 to 0.24927, saving model to BC_classification_prune/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.40112 to 0.24927, saving model to BC_classification_prune/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 2: saving model to BC_classification_prune/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 2: saving model to BC_classification_prune/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "382/382 [==============================] - 1s 3ms/step - loss: 0.2776 - accuracy: 0.9450 - val_loss: 0.2493 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "376/382 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9468\n",
      "***callbacks***\n",
      "saving losses to BC_classification_prune/losses.log\n",
      "\n",
      "Epoch 3: val_loss improved from 0.24927 to 0.18025, saving model to BC_classification_prune/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.24927 to 0.18025, saving model to BC_classification_prune/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 3: saving model to BC_classification_prune/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 3: saving model to BC_classification_prune/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "382/382 [==============================] - 1s 3ms/step - loss: 0.1761 - accuracy: 0.9476 - val_loss: 0.1802 - val_accuracy: 0.9479 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "376/382 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9628\n",
      "***callbacks***\n",
      "saving losses to BC_classification_prune/losses.log\n",
      "\n",
      "Epoch 4: val_loss improved from 0.18025 to 0.16894, saving model to BC_classification_prune/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.18025 to 0.16894, saving model to BC_classification_prune/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 4: saving model to BC_classification_prune/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 4: saving model to BC_classification_prune/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "382/382 [==============================] - 1s 3ms/step - loss: 0.1401 - accuracy: 0.9634 - val_loss: 0.1689 - val_accuracy: 0.9479 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "370/382 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9622\n",
      "***callbacks***\n",
      "saving losses to BC_classification_prune/losses.log\n",
      "\n",
      "Epoch 5: val_loss improved from 0.16894 to 0.15713, saving model to BC_classification_prune/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.16894 to 0.15713, saving model to BC_classification_prune/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 5: saving model to BC_classification_prune/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 5: saving model to BC_classification_prune/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "382/382 [==============================] - 1s 3ms/step - loss: 0.1203 - accuracy: 0.9634 - val_loss: 0.1571 - val_accuracy: 0.9479 - lr: 0.0010\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_save_quantized_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m           callbacks \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mcallbacks);\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39madam, loss\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m], metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel_save_quantized_weights\u001b[49m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_save_quantized_weights' is not defined"
     ]
    }
   ],
   "source": [
    "# adam = Adam(lr=0.02)\n",
    "\n",
    "# model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "# callbacks= all_callbacks( outputDir = 'BC_classification_prune')\n",
    "\n",
    "# model.fit(X_train, Y_train, batch_size=1,\n",
    "#           epochs=5,validation_split=0.2, verbose=1, shuffle=True,\n",
    "#           callbacks = callbacks.callbacks);\n",
    "# model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "# model_save_quantized_weights(model, \"test_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.1004 - accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"Breast_Cancer_model\")\n",
    "accuracy=model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1=10\n",
    "layer_2=5\n",
    "layer_3=3\n",
    "layer_4=2\n",
    "int_bits=0\n",
    "sign_bit=1\n",
    "bits=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import estimate as es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'estimate' from '/home/edge/Desktop/argykokk/hls4ml-tutorial/networks/breast_cancer/estimate.py'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=1\n",
    "reuse=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero weight are:  0\n",
      "Mul ins = 26 and Max muls = 1 and Saved muls = 25 Reuse factor = 100\n",
      "Muxes LUTS: 325\n",
      "LUT cost1=: 62 cost2= 371 bias acc= 70 mult acc630\n",
      "LUTs prediction: 1133\n",
      "Initial muls: 50 Real muls: 50 Initial neurons: 5 Tuned neurons: 5\n",
      "FFs prediction: 1672\n",
      "zero weight are:  0\n",
      "Mul ins = 7 and Max muls = 1 and Saved muls = 6 Reuse factor = 100\n",
      "Muxes LUTS: 78\n",
      "LUT cost1=: 62 cost2= 110 bias acc= 42 mult acc168\n",
      "LUTs prediction: 382\n",
      "Initial muls: 15 Real muls: 15 Initial neurons: 3 Tuned neurons: 3\n",
      "FFs prediction: 903\n",
      "zero weight are:  0\n",
      "Mul ins = 1 and Max muls = 1 and Saved muls = 0 Reuse factor = 100\n",
      "Muxes LUTS: 0\n",
      "LUT cost1=: 62 cost2= 56 bias acc= 28 mult acc56\n",
      "LUTs prediction: 202\n",
      "Initial muls: 6 Real muls: 6 Initial neurons: 2 Tuned neurons: 2\n",
      "FFs prediction: 734\n",
      "CPU times: user 21.4 ms, sys: 3.46 ms, total: 24.9 ms\n",
      "Wall time: 24.3 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#input_num, neurons_num, layer_id, model\n",
    "luts, ffs = es.estimate(10,5,0,model,reuse,param)\n",
    "luts, ffs = es.estimate(5,3,2,model,reuse,param)\n",
    "luts, ffs = es.estimate(3,2,4,model,reuse,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/hls4ml/converters/__init__.py:24: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "None\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: fc1_input, layer type: InputLayer, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 5]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 5]], output shape: [None, 5]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 3]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 3]], output shape: [None, 3]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 3]], output shape: [None, 2]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 2]], output shape: [None, 2]\n",
      "-----------------------------------\n",
      "Model\n",
      "  Precision:         fixed<16,6>\n",
      "  ReuseFactor:       1\n",
      "  Strategy:          Latency\n",
      "  BramFactor:        1000000000\n",
      "  TraceOutput:       False\n",
      "LayerName\n",
      "  fc1_input\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  fc1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,1>\n",
      "      bias:          fixed<8,1>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  fc1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  relu1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<8,0,RND_CONV,SAT>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  fc2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,1>\n",
      "      bias:          fixed<8,1>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  fc2_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  relu2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<8,0,RND_CONV,SAT>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  output\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,1>\n",
      "      bias:          fixed<8,1>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  output_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  softmax\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "      exp_table:     fixed<18,8,RND,SAT>\n",
      "      inv_table:     fixed<18,8,RND,SAT>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Implementation:  stable\n",
      "    Skip:            False\n",
      "    exp_table_t:     ap_fixed<18,8>\n",
      "    inv_table_t:     ap_fixed<18,4>\n",
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: fc1_input, layer type: InputLayer, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 5]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 5]], output shape: [None, 5]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 3]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 3]], output shape: [None, 3]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 3]], output shape: [None, 2]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 2]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "7/7 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='Breast_cancer/hls4ml_prj', part='xc7z007s-clg225-2'\n",
    ")\n",
    "hls_model.compile()\n",
    "\n",
    "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
