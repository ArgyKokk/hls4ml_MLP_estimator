{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 22:52:02.582052: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 22:52:02.732170: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-06 22:52:02.737530: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:02.737542: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-06 22:52:02.763466: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-06 22:52:03.362250: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:03.362311: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:03.362316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from callbacks import all_callbacks\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "import tensorflow.compat.v1 as tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/X_test.npy', 'rb') as f:\n",
    "    X_test = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 22:52:27.034546: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:27.034671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:27.034755: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:27.034834: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:27.034910: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:27.034996: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:27.035074: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:27.035157: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:\n",
      "2023-05-06 22:52:27.035168: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-06 22:52:27.035532: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"Breast_Cancer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07959327101707458, 0.9658536314964294]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0625,  0.5   ,  0.25  ],\n",
       "        [ 0.125 ,  0.125 ,  1.    ],\n",
       "        [-0.5   , -0.25  ,  1.    ],\n",
       "        [-0.5   , -1.    ,  1.    ],\n",
       "        [ 0.25  , -0.5   ,  1.    ],\n",
       "        [ 0.125 , -1.    ,  0.5   ],\n",
       "        [-0.5   , -1.    ,  1.    ],\n",
       "        [-0.25  ,  0.25  ,  0.25  ],\n",
       "        [-0.5   , -0.5   ,  1.    ],\n",
       "        [-0.0625, -0.5   ,  0.5   ]], dtype=float32),\n",
       " array([1.   , 1.   , 0.125], dtype=float32),\n",
       " array([[ 1., -1.],\n",
       "        [ 1., -1.],\n",
       "        [-1.,  1.]], dtype=float32),\n",
       " array([ 1., -1.], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./breast-cancer-wi\n",
    "                 sconsin.csv', sep = ';')\n",
    "print (np.shape(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Y', axis = 1).values\n",
    "y = df.Y\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_reshaped = [[label] for label in y]\n",
    "Y_encoded = encoder.fit_transform(Y_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "y = onehotencoder.fit_transform(y).toarray()\n",
    "#le = LabelEncoder()\n",
    "#print(y)\n",
    "#y = le.fit_transform(y)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, 2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y_encoded,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range=(0,0.9))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/X_train.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "with open('./dataset/X_test.npy', 'wb') as f1:\n",
    "    np.save(f1, X_test)\n",
    "with open('./dataset/Y_test.npy', 'wb') as f2:\n",
    "    np.save(f2, Y_test)\n",
    "with open('./dataset/Y_train.npy', 'wb') as f3:\n",
    "    np.save(f3, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 2**4\n",
    "sc = MinMaxScaler(feature_range=(0,0.9))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "for i in range(0,len(X_train)):\n",
    "   X_train[i] = [int(x*norm)/norm for x in X_train[i]]\n",
    "for i in range(0,len(X_test)):\n",
    "   X_test[i] = [int(x*norm)/norm for x in X_test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=np.argmax(Y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=np.argmax(Y_train,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train: \",X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \",Y_train.shape)\n",
    "print(\"Shape of y_test\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# clf = MLPClassifier(beta_1=0.004044058262057914, beta_2=0.2692099545241596,\n",
    "#               epsilon=0.4100816459563625, hidden_layer_sizes=3, max_iter=150,\n",
    "#               momentum=0.8221177331942455, nesterovs_momentum=False,\n",
    "#               solver='lbfgs', validation_fraction=0.511318982546456,alpha=0.0001).fit(X_train, Y_train)\n",
    "# clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(clf,'./Seeds_argykokk_clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('./Breast_Cancer.MLP_clf.joblib')\n",
    "#clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store the weights, biases of the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "clf = MLPClassifier(beta_1=0.8509567741744859, beta_2=0.9614326787369897,\n",
    "              epsilon=0.1104117717586079, hidden_layer_sizes=3, max_iter=150,\n",
    "              momentum=0.9979940009356586, nesterovs_momentum=False,\n",
    "              solver='sgd', validation_fraction=0.6033229922907446).fit(X_train, Y_train)\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=clf.coefs_[0]\n",
    "b1=clf.intercepts_[0]\n",
    "w2=clf.coefs_[1]\n",
    "b2=clf.intercepts_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wb1=[]\n",
    "wb1.append(w1)\n",
    "wb1.append(b1)\n",
    "\n",
    "wb2=[]\n",
    "wb2.append(w2)\n",
    "wb2.append(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu,quantized_po2\n",
    "import tensorflow.compat.v1 as tf1\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from qkeras.utils import model_save_quantized_weights\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(QDense(3, input_shape=(10,), name='fc1', kernel_quantizer=quantized_bits(8,0,alpha=1,use_stochastic_rounding=True),bias_quantizer=quantized_bits(8,0,alpha=1),\n",
    "                kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)   ))\n",
    "model.add(QActivation(activation=quantized_relu(8,0,use_stochastic_rounding=False), name='relu1'))\n",
    "model.add(QDense(2, name='output',\n",
    "                kernel_quantizer=quantized_bits(8,0,alpha=2,use_stochastic_rounding=True), bias_quantizer=quantized_bits(8,0,alpha=1),\n",
    "                kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights(wb1)\n",
    "model.layers[2].set_weights(wb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.02)\n",
    "\n",
    "model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "callbacks= all_callbacks( outputDir = 'seeds_classification_prune')\n",
    "callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "model.fit(X_train, Y_train, batch_size=1,\n",
    "          epochs=5,validation_split=0.2, verbose=1, shuffle=True,\n",
    "          callbacks = callbacks.callbacks);\n",
    "model = strip_pruning(model)\n",
    "model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "model_save_quantized_weights(model, \"test_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import blackbox as bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bb)\n",
    "weight_size_f1=8\n",
    "bias_size_f1=8\n",
    "weight_size_f2=8\n",
    "bias_size_f2=8\n",
    "relusize_f=8\n",
    "reluint=2\n",
    "input_s=4\n",
    "layer_1=10\n",
    "layer_2=3\n",
    "layer_3=1\n",
    "\n",
    "accuracy, weights = bb.blackbox(wb1, wb2, relusize_f, weight_size_f1, bias_size_f1, weight_size_f2, bias_size_f2, 0, 4, reluint, 10, 3, 2, X_test, Y_test, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### genetic algorithm to determine the relu size, weight size, bias size and sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "import tensorflow.compat.v1 as tf1\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from qkeras.utils import model_save_quantized_weights\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import hls4ml\n",
    "import write_mlp_mergemult_ps as wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import blackbox as bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bb)\n",
    "weight_size_f1=7\n",
    "bias_size_f1=7\n",
    "weight_size_f2=7\n",
    "bias_size_f2=7\n",
    "relusize_f=8\n",
    "reluint=1\n",
    "input_s=4\n",
    "\n",
    "\n",
    "accuracy, weights = bb.blackbox(wb1, wb2, relusize_f, weight_size_f1, bias_size_f1, weight_size_f2, bias_size_f2, 0, 4, reluint, 7, 3, 3, X_test, Y_test, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.quantizers import quantized_bits, quantized_relu,quantized_po2\n",
    "\n",
    "#weights_all=[]\n",
    "#accuracy_all=[]\n",
    "weight_size_f1=4\n",
    "bias_size_f1=3\n",
    "weight_size_f2=6\n",
    "bias_size_f2=4\n",
    "relusize_f=4\n",
    "int_relu=1\n",
    "input_s=4\n",
    "int_w1=0\n",
    "int_b1=0\n",
    "int_w2=0\n",
    "int_b2=0\n",
    "\n",
    "sparsity_val = 0.4 #float(sol[5]/10)\n",
    "#relusize_f = int(sol[0])\n",
    "#weight_size_f1 = int(sol[1])\n",
    "#bias_size_f1 = int(sol[2])\n",
    "#weight_size_f2 = int(sol[3])\n",
    "#bias_size_f2 = int(sol[4])\n",
    "#input_s = int(sol[6])\n",
    "#int_relu = int(sol[7])\n",
    "#int_w1=0\n",
    "#int_b1=0\n",
    "#int_w2=0\n",
    "#int_b2=0\n",
    "#input_size = int(input_s)\n",
    "\n",
    "model = Sequential()\n",
    "layer1=7\n",
    "layer2=3\n",
    "layer3=3\n",
    "i=99\n",
    "epochs=14\n",
    "lr=0.003\n",
    "X = df.drop('Y', axis = 1).values\n",
    "y = df.Y\n",
    "le = LabelEncoder()\n",
    "print(y)\n",
    "y = le.fit_transform(y)\n",
    "print(le.classes_)\n",
    "y = to_categorical(y, 3)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "print(X_train.shape)\n",
    "# reload(pv)\n",
    "# model=pv.generate(i,relusize_f,weight_size_f1,bias_size_f1,weight_size_f2,bias_size_f2,sparsity,inputsize,relusize_int,layer1,layer2,layer3,lr,epochs,X_train,Y_train,X_test,Y_test,wb1,wb2)\n",
    "norm = 2**input_s\n",
    "sc = MinMaxScaler(feature_range=(0,0.9))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "for i in range(0,len(X_train)):\n",
    "    X_train[i] = [int(x*norm)/norm for x in X_train[i]]\n",
    "for i in range(0,len(X_test)):\n",
    "    X_test[i] = [int(x*norm)/norm for x in X_test[i]]\n",
    "\n",
    "\n",
    "print(\"INPUT IS \" + str(input_s)+ \"NORM IS \" + str(norm)+ \"RELU IS \"+str(relusize_f)+\" INT RELU\" +str(int_relu)+\" WEIGHTS 1 ARE \"+str(weight_size_f1)+\" BIASES 1 ARE \"+str(bias_size_f1)+\" WEIGHTS 2 ARE \"+str(weight_size_f2)+\" BIASES 2 ARE \"+str(bias_size_f2)+\" SPARSITY : \"+str(sparsity_val))\n",
    "\n",
    "weight_bias_size=[ [ (weight_size_f1,1), (bias_size_f1,1) ], [ (weight_size_f2,1), (bias_size_f2,1) ] ]\n",
    "relu_size=(relusize_f,int_relu)\n",
    "\n",
    "model.add(QDense(layer2, input_shape=(layer1,), name='fc1', kernel_quantizer=quantized_bits(weight_bias_size[0][0][0],0,alpha=1,use_stochastic_rounding=True),bias_quantizer=quantized_bits(weight_bias_size[0][1][0],0,alpha=1),\n",
    "                kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)   ))\n",
    "model.add(QActivation(activation=quantized_relu(relu_size[0],relu_size[1],use_stochastic_rounding=False), name='relu1'))\n",
    "model.add(QDense(layer3, name='output',\n",
    "                kernel_quantizer=quantized_bits(weight_bias_size[1][0][0],0,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(weight_bias_size[1][1][0],0,alpha=1),\n",
    "                kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "model.add(Activation(activation='softmax', name='softmax'))\n",
    "\n",
    "\n",
    "pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(sparsity_val, begin_step=300, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "model.layers[0].set_weights(wb1)\n",
    "model.layers[2].set_weights(wb2)\n",
    "\n",
    "adam = Adam(lr=0.003)\n",
    "\n",
    "model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "callbacks= all_callbacks( outputDir = 'seeds_classification_prune')\n",
    "callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "model.fit(X_train, Y_train, batch_size=1,\n",
    "            epochs=14,validation_split=0.2, verbose=0, shuffle=True,\n",
    "            callbacks = callbacks.callbacks);\n",
    "model = strip_pruning(model)\n",
    "model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "model_save_quantized_weights(model, \"test_weights\")\n",
    "\n",
    "\n",
    "accuracy=model.evaluate(X_test,Y_test)\n",
    "print(model.get_weights())\n",
    "print(\" ACCURACY IS \"+str(accuracy[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackbox as bb\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PM\n",
    "from pymoo.operators.sampling.rnd import IntegerRandomSampling\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.operators.repair.rounding import RoundingRepair\n",
    "import area as ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyProblem(ElementwiseProblem):\n",
    "\n",
    "    def __init__(self,weightsbiases1,weightsbiases2,layer1,layer2,layer3,X_test,X_train,Y_test,Y_train):\n",
    "        self.weightsbiases1=weightsbiases1\n",
    "        self.weightsbiases2=weightsbiases2\n",
    "        self.layer1=layer1\n",
    "        self.layer2=layer2\n",
    "        self.layer3=layer3\n",
    "        self.X_test=X_test\n",
    "        self.X_train=X_train\n",
    "        self.Y_test=Y_test\n",
    "        self.Y_train=Y_train\n",
    "        #x[0]: relu_size\n",
    "        #x[1]: weight size layer1\n",
    "        #x[2]: bias size layer1\n",
    "        #x[3]: weight size layer2\n",
    "        #x[4]: bias size layer2\n",
    "        #x[5]: pruning sparsity\n",
    "        #x[6]: input size\n",
    "        #x[7]: int relu size\n",
    "        \n",
    "        super().__init__(n_var=8,\n",
    "                         n_obj=2,\n",
    "                         n_ieq_constr=0,\n",
    "                         xl=np.array([3,2,2,2,2,0,2,0]),\n",
    "                         xu=np.array([7,7,5,7,5,8,4,1]),\n",
    "                         vtype=int)\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        accuracy, weights = bb.blackbox(self.weightsbiases1,self.weightsbiases2, x[0], x[1], x[2], x[3], x[4], x[5], x[6] , x[7],  self.layer1, self.layer2, self.layer3, self.X_test, self.Y_test, self.X_train, self.Y_train)\n",
    "        f1 = 1- accuracy\n",
    "        f2 = ar.area(weights,x[6],x[0],x[1],x[3],self.layer1,self.layer2,self.layer3)\n",
    "\n",
    "        out[\"F\"] = [f1, f2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1=7\n",
    "layer2=3\n",
    "layer3=3\n",
    "problem = MyProblem(wb1,wb2,layer1,layer2,layer3,X_test,X_train,Y_test,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algorithm = NSGA2(\n",
    "    pop_size=80,\n",
    "    n_offsprings=60,\n",
    "    sampling=IntegerRandomSampling(),\n",
    "    crossover=SBX(vtype=float, repair=RoundingRepair()),\n",
    "    mutation=PM( vtype=float, repair=RoundingRepair()),\n",
    "    eliminate_duplicates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.termination import get_termination\n",
    "\n",
    "termination = get_termination(\"n_gen\",60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "res = minimize(problem,\n",
    "               algorithm,\n",
    "               termination,\n",
    "               seed=1,\n",
    "               save_history=True,\n",
    "               verbose=True)\n",
    "X = res.X\n",
    "F = res.F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-res.F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot all the solutions from the final generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.visualization.scatter import Scatter\n",
    "pop=res.pop\n",
    "vals=pop.get(\"F\")\n",
    "plot = Scatter()\n",
    "plot.add(problem.pareto_front(), plot_type=\"line\", color=\"black\", alpha=0.7)\n",
    "plot.add(vals, facecolor=\"none\", edgecolor=\"red\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the pareto solutions from the final generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = Scatter()\n",
    "plot.add(problem.pareto_front(), plot_type=\"line\", color=\"black\", alpha=0.7)\n",
    "plot.add(val, facecolor=\"none\", edgecolor=\"blue\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1=7\n",
    "layer_2=3\n",
    "layer_3=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"paretos_seeds\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(res.X, fp)\n",
    "# with open(\"costs_seeds\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(res.F, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paretos_seeds\", \"rb\") as fp:   # Unpickling\n",
    "    paretos = pickle.load(fp)\n",
    "with open(\"costs_seeds\", \"rb\") as fp:   # Unpickling\n",
    "    costs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start creating the verilogs for :\n",
    "#### i) the found Pareto\n",
    "#### ii) only pruning sparsity range [10-80]\n",
    "#### iii) weight clustering (the open-source python framework for compression of DNNs)\n",
    "#### iv) custom weight sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pareto_verilogs as pv\n",
    "import pareto_clustering_verilogs as pcv\n",
    "import only_pruning_verilog as opv\n",
    "from importlib import reload\n",
    "reload(pv)\n",
    "reload(pcv)\n",
    "reload(opv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "layer1=7\n",
    "layer2=3\n",
    "layer3=3\n",
    "epochs=14\n",
    "lr=0.003\n",
    "opv.generate(i,layer1,layer2,layer3,lr,epochs,X_train,Y_train,X_test,Y_test,wb1,wb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import weightsharing as ws\n",
    "# from importlib import reload\n",
    "\n",
    "# weights_all=[]\n",
    "# accuracy_all=[]\n",
    "# for sol in true_paretos:\n",
    "\n",
    "#     sparsity_val = float(sol[5]/10)\n",
    "#     relusize_f = int(sol[0])\n",
    "#     weight_size_f1 = int(sol[1])\n",
    "#     bias_size_f1 = int(sol[2])\n",
    "#     weight_size_f2 = int(sol[3])\n",
    "#     bias_size_f2 = int(sol[4])\n",
    "#     input_s = int(sol[6])\n",
    "#     int_relu = int(sol[7])\n",
    "#     int_w1=0\n",
    "#     int_b1=0\n",
    "#     int_w2=0\n",
    "#     int_b2=0\n",
    "#     input_size = int(input_s)\n",
    "\n",
    "\n",
    "#     model = Sequential()\n",
    "#     layer1=7\n",
    "#     layer2=3\n",
    "#     layer3=3\n",
    "#     i=99\n",
    "#     epochs=14\n",
    "#     lr=0.003\n",
    "#     X = df.drop('Y', axis = 1).values\n",
    "#     y = df.Y\n",
    "#     le = LabelEncoder()\n",
    "#     print(y)\n",
    "#     y = le.fit_transform(y)\n",
    "#     print(le.classes_)\n",
    "#     y = to_categorical(y, 3)\n",
    "#     X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "#     print(X_train.shape)\n",
    "#     # reload(pv)\n",
    "#     # model=pv.generate(i,relusize_f,weight_size_f1,bias_size_f1,weight_size_f2,bias_size_f2,sparsity,inputsize,relusize_int,layer1,layer2,layer3,lr,epochs,X_train,Y_train,X_test,Y_test,wb1,wb2)\n",
    "#     norm = 2**input_s\n",
    "#     sc = MinMaxScaler(feature_range=(0,0.9))\n",
    "#     X_train = sc.fit_transform(X_train)\n",
    "#     X_test = sc.transform(X_test)\n",
    "#     for i in range(0,len(X_train)):\n",
    "#         X_train[i] = [int(x*norm)/norm for x in X_train[i]]\n",
    "#     for i in range(0,len(X_test)):\n",
    "#         X_test[i] = [int(x*norm)/norm for x in X_test[i]]\n",
    "\n",
    "\n",
    "#     print(\"INPUT IS \" + str(input_s)+ \"NORM IS \" + str(norm)+ \"RELU IS \"+str(relusize_f)+\" INT RELU\" +str(int_relu)+\" WEIGHTS 1 ARE \"+str(weight_size_f1)+\" BIASES 1 ARE \"+str(bias_size_f1)+\" WEIGHTS 2 ARE \"+str(weight_size_f2)+\" BIASES 2 ARE \"+str(bias_size_f2)+\" SPARSITY : \"+str(sparsity_val))\n",
    "\n",
    "#     weight_bias_size=[ [ (weight_size_f1,1), (bias_size_f1,1) ], [ (weight_size_f2,1), (bias_size_f2,1) ] ]\n",
    "#     relu_size=(relusize_f,int_relu)\n",
    "\n",
    "#     model.add(QDense(layer2, input_shape=(layer1,), name='fc1', kernel_quantizer=quantized_bits(weight_bias_size[0][0][0],0,alpha=1,use_stochastic_rounding=True),bias_quantizer=quantized_bits(weight_bias_size[0][1][0],0,alpha=1),\n",
    "#                     kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)   ))\n",
    "#     model.add(QActivation(activation=quantized_relu(relu_size[0],relu_size[1],use_stochastic_rounding=False), name='relu1'))\n",
    "#     model.add(QDense(layer3, name='output',\n",
    "#                     kernel_quantizer=quantized_bits(weight_bias_size[1][0][0],0,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(weight_bias_size[1][1][0],0,alpha=1),\n",
    "#                     kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "#     model.add(Activation(activation='softmax', name='softmax'))\n",
    "\n",
    "\n",
    "#     pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(sparsity_val, begin_step=300, frequency=100)}\n",
    "#     model = prune.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "#     model.layers[0].set_weights(wb1)\n",
    "#     model.layers[2].set_weights(wb2)\n",
    "\n",
    "#     adam = Adam(lr=0.003)\n",
    "\n",
    "#     model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "#     callbacks= all_callbacks( outputDir = 'seeds_classification_prune')\n",
    "#     callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "#     model.fit(X_train, Y_train, batch_size=1,\n",
    "#                 epochs=14,validation_split=0.2, verbose=0, shuffle=True,\n",
    "#                 callbacks = callbacks.callbacks);\n",
    "#     model = strip_pruning(model)\n",
    "#     model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "#     model_save_quantized_weights(model, \"test_weights\")\n",
    "\n",
    "\n",
    "#     accuracy=model.evaluate(X_test,Y_test)\n",
    "#     print(model.get_weights())\n",
    "#     print(\" ACCURACY IS \"+str(accuracy[1]) )\n",
    "#     accuracy_all.append(accuracy[1])\n",
    "#     weights_all.append(model.trainable_variables)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import weightsharing as ws\n",
    "# from importlib import reload\n",
    "# import copy\n",
    "\n",
    "# #weights_all=[]\n",
    "# #accuracy_all=[]\n",
    "# sol=copy.deepcopy(true_paretos[0])\n",
    "\n",
    "# sparsity_val = float(sol[5]/10)\n",
    "# relusize_f = int(sol[0])\n",
    "# weight_size_f1 = int(sol[1])\n",
    "# bias_size_f1 = int(sol[2])\n",
    "# weight_size_f2 = int(sol[3])\n",
    "# bias_size_f2 = int(sol[4])\n",
    "# input_s = int(sol[6])\n",
    "# int_relu = int(sol[7])\n",
    "# int_w1=0\n",
    "# int_b1=0\n",
    "# int_w2=0\n",
    "# int_b2=0\n",
    "# input_size = int(input_s)\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# layer1=7\n",
    "# layer2=3\n",
    "# layer3=3\n",
    "# i=99\n",
    "# epochs=14\n",
    "# lr=0.003\n",
    "# X = df.drop('Y', axis = 1).values\n",
    "# y = df.Y\n",
    "# le = LabelEncoder()\n",
    "# print(y)\n",
    "# y = le.fit_transform(y)\n",
    "# print(le.classes_)\n",
    "# y = to_categorical(y, 3)\n",
    "# X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "# print(X_train.shape)\n",
    "# # reload(pv)\n",
    "# # model=pv.generate(i,relusize_f,weight_size_f1,bias_size_f1,weight_size_f2,bias_size_f2,sparsity,inputsize,relusize_int,layer1,layer2,layer3,lr,epochs,X_train,Y_train,X_test,Y_test,wb1,wb2)\n",
    "# norm = 2**input_s\n",
    "# sc = MinMaxScaler(feature_range=(0,0.9))\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "# for i in range(0,len(X_train)):\n",
    "#     X_train[i] = [int(x*norm)/norm for x in X_train[i]]\n",
    "# for i in range(0,len(X_test)):\n",
    "#     X_test[i] = [int(x*norm)/norm for x in X_test[i]]\n",
    "\n",
    "\n",
    "# print(\"INPUT IS \" + str(input_s)+ \"NORM IS \" + str(norm)+ \"RELU IS \"+str(relusize_f)+\" INT RELU\" +str(int_relu)+\" WEIGHTS 1 ARE \"+str(weight_size_f1)+\" BIASES 1 ARE \"+str(bias_size_f1)+\" WEIGHTS 2 ARE \"+str(weight_size_f2)+\" BIASES 2 ARE \"+str(bias_size_f2)+\" SPARSITY : \"+str(sparsity_val))\n",
    "\n",
    "# weight_bias_size=[ [ (weight_size_f1,1), (bias_size_f1,1) ], [ (weight_size_f2,1), (bias_size_f2,1) ] ]\n",
    "# relu_size=(relusize_f,int_relu)\n",
    "\n",
    "# model.add(QDense(layer2, input_shape=(layer1,), name='fc1', kernel_quantizer=quantized_bits(weight_bias_size[0][0][0],0,alpha=1,use_stochastic_rounding=True),bias_quantizer=quantized_bits(weight_bias_size[0][1][0],0,alpha=1),\n",
    "#                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)   ))\n",
    "# model.add(QActivation(activation=quantized_relu(relu_size[0],relu_size[1],use_stochastic_rounding=False), name='relu1'))\n",
    "# model.add(QDense(layer3, name='output',\n",
    "#                 kernel_quantizer=quantized_bits(weight_bias_size[1][0][0],0,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(weight_bias_size[1][1][0],0,alpha=1),\n",
    "#                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "# model.add(Activation(activation='softmax', name='softmax'))\n",
    "\n",
    "\n",
    "# pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(sparsity_val, begin_step=300, frequency=100)}\n",
    "# model = prune.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# model.layers[0].set_weights(wb1)\n",
    "# model.layers[2].set_weights(wb2)\n",
    "\n",
    "# adam = Adam(lr=0.003)\n",
    "\n",
    "# model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "# callbacks= all_callbacks( outputDir = 'seeds_classification_prune')\n",
    "# callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "# model.fit(X_train, Y_train, batch_size=1,\n",
    "#             epochs=14,validation_split=0.2, verbose=0, shuffle=True,\n",
    "#             callbacks = callbacks.callbacks);\n",
    "# model = strip_pruning(model)\n",
    "# model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "# model_save_quantized_weights(model, \"test_weights\")\n",
    "\n",
    "\n",
    "# accuracy=model.evaluate(X_test,Y_test)\n",
    "# print(model.get_weights())\n",
    "# print(\" ACCURACY IS \"+str(accuracy[1]) )\n",
    "# accuracy_all.append(accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.set_weights(weights_all[0])\n",
    "# accuracy=model.evaluate(X_test,Y_test)\n",
    "# print(\" ACCURACY IS \"+str(accuracy[1]) )\n",
    "# accuracy_all.append(accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for solution in paretos:\n",
    "    relusize_f=solution[0]\n",
    "    weight_size_f1=solution[1]\n",
    "    bias_size_f1=solution[2]\n",
    "    weight_size_f2=solution[3]\n",
    "    bias_size_f2=solution[4]\n",
    "    sparsity=solution[5]\n",
    "    inputsize=solution[6]\n",
    "    relusize_int=solution[7]\n",
    "    model=pv.generate(i,relusize_f,weight_size_f1,bias_size_f1,weight_size_f2,bias_size_f2,sparsity,inputsize,relusize_int,layer1,layer2,layer3,lr,epochs,X_train,Y_train,X_test,Y_test,wb1,wb2)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pcv)\n",
    "i=0\n",
    "relusize_f=8#solution[0]\n",
    "weight_size_f1=7#solution[1]\n",
    "bias_size_f1=7#solution[2]\n",
    "weight_size_f2=7#solution[3]\n",
    "bias_size_f2=7#solution[4]\n",
    "sparsity=0#solution[5]\n",
    "inputsize=4#solution[6]\n",
    "relusize_int=1#solution[7]\n",
    "model=pv.generate(i,relusize_f,weight_size_f1,bias_size_f1,weight_size_f2,bias_size_f2,sparsity,inputsize,relusize_int,layer1,layer2,layer3,lr,epochs,X_train,Y_train,X_test,Y_test,wb1,wb2)\n",
    "pcv.generate(model,i,relusize_f,weight_size_f1,bias_size_f1,weight_size_f2,bias_size_f2,inputsize,relusize_int,layer1,layer2,layer3,X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"paretos_seeds\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(true_paretos, fp)\n",
    "# with open(\"weights_list_true_paretos_seeds\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(weights_all, fp)\n",
    "# with open(\"accuracies_true_paretos_seeds\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(accuracy_all, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paretos_seeds\", \"rb\") as fp:   # Unpickling\n",
    "    true_paretos = pickle.load(fp)\n",
    "with open(\"weights_list_true_paretos_seeds\", \"rb\") as fp:   # Unpickling\n",
    "    weights_all = pickle.load(fp)\n",
    "with open(\"accuracies_true_paretos_seeds\", \"rb\") as fp:   # Unpickling\n",
    "    accuracy_all = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom weight sharing on Pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pareto_sharing as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(0,len(true_paretos)):\n",
    "    reload(ps)\n",
    "    relu= int(true_paretos[w][0])\n",
    "    bits_l1= int(true_paretos[w][1])\n",
    "    bias_l1= int(true_paretos[w][2])\n",
    "    bits_l2=  int(true_paretos[w][3])\n",
    "    bias_l2=  int(true_paretos[w][4])\n",
    "    input_s= int(true_paretos[w][6])\n",
    "    relu_int = int(true_paretos[w][7]) + 1\n",
    "    bits_int_l1= 1\n",
    "    bias_int_l1= 1\n",
    "    bits_int_l2 = 1\n",
    "    bias_int_l2 = 1\n",
    "    layer1=7\n",
    "    layer2=3\n",
    "    layer3=3\n",
    "\n",
    "    window=0\n",
    "    for i in range(1,layer2+1):\n",
    "        for j in range(1,layer3+1):\n",
    "            c1 = i\n",
    "            c2 = j\n",
    "            ps.top_sharing(w,weights_all[w], bits_l1, bits_int_l1, bits_l2, bits_int_l2, bias_l1, bias_int_l1, bias_l2, bias_int_l2, input_s, relu, relu_int, X_test, Y_test, layer1, layer2, layer3, c1, c2, window)\n",
    "    window=20\n",
    "    for i in range(1,layer2+1):\n",
    "        for j in range(1,layer3+1):\n",
    "            c1 = i\n",
    "            c2 = j\n",
    "            ps.top_sharing(w,weights_all[w], bits_l1, bits_int_l1, bits_l2, bits_int_l2, bias_l1, bias_int_l1, bias_l2, bias_int_l2, input_s, relu, relu_int, X_test, Y_test, layer1, layer2, layer3, c1, c2, window)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
