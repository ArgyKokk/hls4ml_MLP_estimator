{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 02:30:32.593771: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-16 02:30:32.684264: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = '/tools/Xilinx/Vitis/2022.1/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "data = fetch_openml('hls4ml_lhc_jets_hlf')\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y, 5)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a model\n",
    "This time we're going to use QKeras layers.\n",
    "QKeras is \"Quantized Keras\" for deep heterogeneous quantization of ML models.\n",
    "\n",
    "https://github.com/google/qkeras\n",
    "\n",
    "It is maintained by Google and we recently added support for QKeras model to hls4ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using `QDense` layer instead of `Dense`, and `QActivation` instead of `Activation`. We're also specifying `kernel_quantizer = quantized_bits(6,0,0)`. This will use 6-bits (of which 0 are integer) for the weights. We also use the same quantization for the biases, and `quantized_relu(6)` for 6-bit ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 02:30:54.201239: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "layer_1=16\n",
    "layer_2=64\n",
    "layer_3=32\n",
    "layer_4=32\n",
    "layer_5=5\n",
    "int_bits=0\n",
    "sign_bit=1\n",
    "bits=8\n",
    "model = Sequential()\n",
    "model.add(QDense(layer_2, input_shape=(layer_1,), name='fc1', kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True),bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)   ))\n",
    "model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu1'))\n",
    "model.add(QDense(layer_3, name='fc2',\n",
    "                kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu2'))\n",
    "model.add(QDense(layer_4, name='fc3',\n",
    "                kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu3'))\n",
    "model.add(QDense(layer_5, name='output',\n",
    "                kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "                kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "483/487 [============================>.] - ETA: 0s - loss: 0.9670 - accuracy: 0.6804\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.81191, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.81191, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 1: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 1: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 9s 12ms/step - loss: 0.9658 - accuracy: 0.6808 - val_loss: 0.8119 - val_accuracy: 0.7371 - lr: 0.0010\n",
      "Epoch 2/14\n",
      "487/487 [==============================] - ETA: 0s - loss: 0.7781 - accuracy: 0.7467\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 2: val_loss improved from 0.81191 to 0.76310, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.81191 to 0.76310, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 2: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 2: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7781 - accuracy: 0.7467 - val_loss: 0.7631 - val_accuracy: 0.7510 - lr: 0.0010\n",
      "Epoch 3/14\n",
      "486/487 [============================>.] - ETA: 0s - loss: 0.7494 - accuracy: 0.7543\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 3: val_loss improved from 0.76310 to 0.74700, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.76310 to 0.74700, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 3: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 3: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7494 - accuracy: 0.7543 - val_loss: 0.7470 - val_accuracy: 0.7551 - lr: 0.0010\n",
      "Epoch 4/14\n",
      "482/487 [============================>.] - ETA: 0s - loss: 0.7360 - accuracy: 0.7575\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 4: val_loss improved from 0.74700 to 0.73663, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.74700 to 0.73663, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 4: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 4: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 11ms/step - loss: 0.7361 - accuracy: 0.7576 - val_loss: 0.7366 - val_accuracy: 0.7575 - lr: 0.0010\n",
      "Epoch 5/14\n",
      "484/487 [============================>.] - ETA: 0s - loss: 0.7271 - accuracy: 0.7596\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 5: val_loss improved from 0.73663 to 0.72796, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.73663 to 0.72796, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 5: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 5: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 11ms/step - loss: 0.7270 - accuracy: 0.7596 - val_loss: 0.7280 - val_accuracy: 0.7594 - lr: 0.0010\n",
      "Epoch 6/14\n",
      "482/487 [============================>.] - ETA: 0s - loss: 0.7203 - accuracy: 0.7612\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 6: val_loss improved from 0.72796 to 0.72382, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.72796 to 0.72382, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 6: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 6: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7204 - accuracy: 0.7612 - val_loss: 0.7238 - val_accuracy: 0.7599 - lr: 0.0010\n",
      "Epoch 7/14\n",
      "486/487 [============================>.] - ETA: 0s - loss: 0.7151 - accuracy: 0.7621\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 7: val_loss improved from 0.72382 to 0.71935, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.72382 to 0.71935, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 7: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 7: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7151 - accuracy: 0.7621 - val_loss: 0.7194 - val_accuracy: 0.7609 - lr: 0.0010\n",
      "Epoch 8/14\n",
      "485/487 [============================>.] - ETA: 0s - loss: 0.7111 - accuracy: 0.7633\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 8: val_loss improved from 0.71935 to 0.71477, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.71935 to 0.71477, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 8: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 8: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 11ms/step - loss: 0.7111 - accuracy: 0.7633 - val_loss: 0.7148 - val_accuracy: 0.7617 - lr: 0.0010\n",
      "Epoch 9/14\n",
      "482/487 [============================>.] - ETA: 0s - loss: 0.7086 - accuracy: 0.7638\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 9: val_loss improved from 0.71477 to 0.71304, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.71477 to 0.71304, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 9: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 9: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 11ms/step - loss: 0.7085 - accuracy: 0.7638 - val_loss: 0.7130 - val_accuracy: 0.7623 - lr: 0.0010\n",
      "Epoch 10/14\n",
      "486/487 [============================>.] - ETA: 0s - loss: 0.7059 - accuracy: 0.7642\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 10: val_loss improved from 0.71304 to 0.71286, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.71304 to 0.71286, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 10: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 10: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 10: saving model to jt_classification/KERAS_check_model_epoch10.h5\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 6s 11ms/step - loss: 0.7059 - accuracy: 0.7642 - val_loss: 0.7129 - val_accuracy: 0.7619 - lr: 0.0010\n",
      "Epoch 11/14\n",
      "482/487 [============================>.] - ETA: 0s - loss: 0.7022 - accuracy: 0.7651\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 11: val_loss improved from 0.71286 to 0.70694, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.71286 to 0.70694, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 11: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 11: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 11ms/step - loss: 0.7021 - accuracy: 0.7652 - val_loss: 0.7069 - val_accuracy: 0.7634 - lr: 5.0000e-04\n",
      "Epoch 12/14\n",
      "481/487 [============================>.] - ETA: 0s - loss: 0.7009 - accuracy: 0.7649\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.70694\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.70694\n",
      "\n",
      "Epoch 12: saving model to jt_classification/KERAS_check_model_last.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.7010 - accuracy: 0.7649 - val_loss: 0.7084 - val_accuracy: 0.7635 - lr: 5.0000e-04\n",
      "Epoch 13/14\n",
      "481/487 [============================>.] - ETA: 0s - loss: 0.6995 - accuracy: 0.7656\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 13: val_loss improved from 0.70694 to 0.70544, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.70694 to 0.70544, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 13: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 13: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 5s 10ms/step - loss: 0.6996 - accuracy: 0.7655 - val_loss: 0.7054 - val_accuracy: 0.7640 - lr: 5.0000e-04\n",
      "Epoch 14/14\n",
      "482/487 [============================>.] - ETA: 0s - loss: 0.6990 - accuracy: 0.7656\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 14: val_loss improved from 0.70544 to 0.70502, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.70544 to 0.70502, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 14: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 14: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "487/487 [==============================] - 6s 12ms/step - loss: 0.6990 - accuracy: 0.7656 - val_loss: 0.7050 - val_accuracy: 0.7635 - lr: 5.0000e-04\n",
      "... quantizing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Jet_tagging/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Jet_tagging/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from qkeras.utils import model_save_quantized_weights\n",
    "\n",
    "pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "\n",
    "model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "callbacks= all_callbacks( outputDir = 'jt_classification')\n",
    "callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "model.fit(X_train, Y_train, batch_size=1024,\n",
    "          epochs=14,validation_split=0.25, verbose=1, shuffle=True,\n",
    "          callbacks = callbacks.callbacks);\n",
    "model = strip_pruning(model)\n",
    "model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "model_save_quantized_weights(model, \"test_weights\")\n",
    "model.save(\"Jet_tagging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overall_LUTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import estimate as es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'estimate' from '/home/edge/Desktop/argykokk/hls4ml-tutorial/networks/jet_tagging/estimate.py'>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=1\n",
    "reuse=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero weight are:  165\n",
      "Mul ins = 247 and Max muls = 9 and Saved muls = 238 Reuse factor = 100\n",
      "Muxes LUTS: 3094\n",
      "LUT cost1=: 558 cost2= 5058 bias acc= 896 mult acc11130\n",
      "LUTs prediction: 17642\n",
      "Initial muls: 1024 Real muls: 859 Initial neurons: 64 Tuned neurons: 54\n",
      "FFs prediction: 16262\n",
      "zero weight are:  930\n",
      "Mul ins = 275 and Max muls = 12 and Saved muls = 263 Reuse factor = 100\n",
      "Muxes LUTS: 3419\n",
      "LUT cost1=: 744 cost2= 7731 bias acc= 448 mult acc15204\n",
      "LUTs prediction: 24127\n",
      "Initial muls: 2048 Real muls: 1118 Initial neurons: 32 Tuned neurons: 18\n",
      "FFs prediction: 22560\n",
      "zero weight are:  438\n",
      "Mul ins = 202 and Max muls = 6 and Saved muls = 196 Reuse factor = 100\n",
      "Muxes LUTS: 2548\n",
      "LUT cost1=: 372 cost2= 4000 bias acc= 448 mult acc7756\n",
      "LUTs prediction: 12576\n",
      "Initial muls: 1024 Real muls: 586 Initial neurons: 32 Tuned neurons: 19\n",
      "FFs prediction: 12181\n",
      "zero weight are:  27\n",
      "Mul ins = 85 and Max muls = 2 and Saved muls = 83 Reuse factor = 100\n",
      "Muxes LUTS: 1079\n",
      "LUT cost1=: 124 cost2= 739 bias acc= 70 mult acc1792\n",
      "LUTs prediction: 2725\n",
      "Initial muls: 160 Real muls: 133 Initial neurons: 5 Tuned neurons: 5\n",
      "FFs prediction: 3944\n",
      "CPU times: user 36.3 ms, sys: 4 ms, total: 40.3 ms\n",
      "Wall time: 43 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#input_num, neurons_num, layer_id, model\n",
    "ffs = es.estimate(16,64,0,model,reuse,param)\n",
    "luts, ffs = es.estimate(64,32,2,model,reuse,param)\n",
    "luts, ffs = es.estimate(32,32,4,model,reuse,param)\n",
    "luts, ffs = es.estimate(32,5,6,model,reuse,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/hls4ml/converters/__init__.py:24: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "None\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: fc1_input, layer type: InputLayer, input shapes: [[None, 16]], output shape: [None, 16]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 16]], output shape: [None, 64]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 64]], output shape: [None, 64]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 64]], output shape: [None, 32]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 5]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 5]], output shape: [None, 5]\n",
      "-----------------------------------\n",
      "Model\n",
      "  Precision:         fixed<16,6>\n",
      "  ReuseFactor:       1\n",
      "  Strategy:          Latency\n",
      "  BramFactor:        1000000000\n",
      "  TraceOutput:       False\n",
      "LayerName\n",
      "  fc1_input\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  fc1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,1>\n",
      "      bias:          fixed<8,1>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  fc1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  relu1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<8,0,RND_CONV,SAT>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  fc2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,1>\n",
      "      bias:          fixed<8,1>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  fc2_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  relu2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<8,0,RND_CONV,SAT>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  fc3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,1>\n",
      "      bias:          fixed<8,1>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  fc3_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  relu3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<8,0,RND_CONV,SAT>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  output\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,1>\n",
      "      bias:          fixed<8,1>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  output_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  softmax\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "      exp_table:     fixed<18,8,RND,SAT>\n",
      "      inv_table:     fixed<18,8,RND,SAT>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Implementation:  stable\n",
      "    Skip:            False\n",
      "    exp_table_t:     ap_fixed<18,8>\n",
      "    inv_table_t:     ap_fixed<18,4>\n",
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: fc1_input, layer type: InputLayer, input shapes: [[None, 16]], output shape: [None, 16]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 16]], output shape: [None, 64]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 64]], output shape: [None, 64]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 64]], output shape: [None, 32]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 5]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 5]], output shape: [None, 5]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "5188/5188 [==============================] - 12s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='jet_tagging/hls4ml_prj', part='xc7z007s-clg225-2'\n",
    ")\n",
    "hls_model.compile()\n",
    "\n",
    "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
