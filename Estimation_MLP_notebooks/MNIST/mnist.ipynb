{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 08:09:38.604417: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 08:09:38.714024: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "os.environ['PATH'] = '/tools/Xilinx/Vitis/2022.1/bin:' + os.environ['PATH']\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "new_size = (14,14)\n",
    "X_train_resized = np.array([resize(image,new_size) for image in x_train])\n",
    "X_test_resized = np.array([resize(image,new_size) for image in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 14, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample 25 mnist digits from train dataset\n",
    "indexes = np.random.randint(0, x_train.shape[0], size=25)\n",
    "images = X_train_resized[indexes]\n",
    "labels = y_train[indexes]\n",
    "\n",
    "\n",
    "# plot the 25 mnist digits\n",
    "plt.figure(figsize=(5,5))\n",
    "for i in range(len(indexes)):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    image = images[i]\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()\n",
    "plt.savefig(\"mnist-samples.png\")\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image dimensions (assumed square)\n",
    "image_size = X_train_resized.shape[1]\n",
    "input_size = image_size * image_size\n",
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train_resized, [-1, input_size])\n",
    "X_test = np.reshape(X_test_resized, [-1, input_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00409751, -0.00911442, -0.01238149, ..., -0.0314962 ,\n",
       "        -0.02067988, -0.00593342],\n",
       "       [-0.00409751, -0.00911442, -0.01238149, ..., -0.0314962 ,\n",
       "        -0.02067988, -0.00593342],\n",
       "       [-0.00409751, -0.00911442, -0.01238149, ..., -0.0314962 ,\n",
       "        -0.02067988, -0.00593342],\n",
       "       ...,\n",
       "       [-0.00409751, -0.00911442, -0.01238149, ..., -0.0314962 ,\n",
       "        -0.02067988, -0.00593342],\n",
       "       [-0.00409751, -0.00911442, -0.01238149, ..., -0.0314962 ,\n",
       "        -0.02067988, -0.00593342],\n",
       "       [-0.00409751, -0.00911442, -0.01238149, ..., -0.0314962 ,\n",
       "        -0.02067988, -0.00593342]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using `QDense` layer instead of `Dense`, and `QActivation` instead of `Activation`. We're also specifying `kernel_quantizer = quantized_bits(6,0,0)`. This will use 6-bits (of which 0 are integer) for the weights. We also use the same quantization for the biases, and `quantized_relu(6)` for 6-bit ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_1=196\n",
    "# layer_2=56\n",
    "# layer_3=64\n",
    "# layer_4=32\n",
    "# layer_5=10\n",
    "# int_bits=1\n",
    "# sign_bit=1\n",
    "# bits=8\n",
    "# model = Sequential()\n",
    "# model.add(QDense(layer_2, input_shape=(layer_1,), name='fc1', kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True),bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "#                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)   ))\n",
    "# model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu1'))\n",
    "# model.add(QDense(layer_3, name='fc2',\n",
    "#                 kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "#                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "# model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu2'))\n",
    "# model.add(QDense(layer_4, name='fc3',\n",
    "#                 kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "#                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "# model.add(QActivation(activation=quantized_relu(bits,int_bits,use_stochastic_rounding=False), name='relu3'))\n",
    "# model.add(QDense(layer_5, name='output',\n",
    "#                 kernel_quantizer=quantized_bits(bits,int_bits,alpha=1,use_stochastic_rounding=True), bias_quantizer=quantized_bits(bits,int_bits,alpha=1),\n",
    "#                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001 ) ))\n",
    "# model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.1587 - accuracy: 0.9866\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.21456, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.21456, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 1: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 1: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 4s 34ms/step - loss: 0.1585 - accuracy: 0.9866 - val_loss: 0.2146 - val_accuracy: 0.9673 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.1534 - accuracy: 0.9875\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 2: val_loss improved from 0.21456 to 0.21440, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.21456 to 0.21440, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 2: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 2: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 22ms/step - loss: 0.1535 - accuracy: 0.9875 - val_loss: 0.2144 - val_accuracy: 0.9677 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.1487 - accuracy: 0.9894\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 3: val_loss improved from 0.21440 to 0.21312, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.21440 to 0.21312, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 3: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 3: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.1495 - accuracy: 0.9891 - val_loss: 0.2131 - val_accuracy: 0.9677 - lr: 5.0000e-04\n",
      "Epoch 4/25\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.1470 - accuracy: 0.9900\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 4: val_loss improved from 0.21312 to 0.21264, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.21312 to 0.21264, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 4: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 4: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1468 - accuracy: 0.9901 - val_loss: 0.2126 - val_accuracy: 0.9679 - lr: 5.0000e-04\n",
      "Epoch 5/25\n",
      "40/44 [==========================>...] - ETA: 0s - loss: 0.1456 - accuracy: 0.9901\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 5: val_loss improved from 0.21264 to 0.21192, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.21264 to 0.21192, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 5: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 5: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.1454 - accuracy: 0.9900 - val_loss: 0.2119 - val_accuracy: 0.9682 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.1438 - accuracy: 0.9905\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.21192\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.21192\n",
      "\n",
      "Epoch 6: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 6: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1439 - accuracy: 0.9905 - val_loss: 0.2120 - val_accuracy: 0.9683 - lr: 5.0000e-04\n",
      "Epoch 7/25\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9916\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 7: val_loss improved from 0.21192 to 0.20954, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.21192 to 0.20954, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 7: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 7: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.1415 - accuracy: 0.9916 - val_loss: 0.2095 - val_accuracy: 0.9681 - lr: 2.5000e-04\n",
      "Epoch 8/25\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.1400 - accuracy: 0.9918\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.20954\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.20954\n",
      "\n",
      "Epoch 8: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 8: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1407 - accuracy: 0.9916 - val_loss: 0.2103 - val_accuracy: 0.9689 - lr: 2.5000e-04\n",
      "Epoch 9/25\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.1399 - accuracy: 0.9920\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 9: val_loss improved from 0.20954 to 0.20827, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.20954 to 0.20827, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 9: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 9: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.1398 - accuracy: 0.9920 - val_loss: 0.2083 - val_accuracy: 0.9691 - lr: 2.5000e-04\n",
      "Epoch 10/25\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.1393 - accuracy: 0.9921\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 10: val_loss improved from 0.20827 to 0.20825, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.20827 to 0.20825, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 10: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 10: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 10: saving model to jt_classification/KERAS_check_model_epoch10.h5\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.1393 - accuracy: 0.9920 - val_loss: 0.2082 - val_accuracy: 0.9684 - lr: 2.5000e-04\n",
      "Epoch 11/25\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.1376 - accuracy: 0.9923\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.20825\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.20825\n",
      "\n",
      "Epoch 11: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 11: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1377 - accuracy: 0.9923 - val_loss: 0.2091 - val_accuracy: 0.9685 - lr: 1.2500e-04\n",
      "Epoch 12/25\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.1370 - accuracy: 0.9927\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 12: val_loss improved from 0.20825 to 0.20802, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.20825 to 0.20802, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 12: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 12: saving model to jt_classification/KERAS_check_model_last_weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.1371 - accuracy: 0.9926 - val_loss: 0.2080 - val_accuracy: 0.9680 - lr: 1.2500e-04\n",
      "Epoch 13/25\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9924\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 13: val_loss improved from 0.20802 to 0.20774, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.20802 to 0.20774, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 13: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 13: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.1370 - accuracy: 0.9924 - val_loss: 0.2077 - val_accuracy: 0.9689 - lr: 1.2500e-04\n",
      "Epoch 14/25\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9924\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.20774\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.20774\n",
      "\n",
      "Epoch 14: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 14: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.1366 - accuracy: 0.9925 - val_loss: 0.2092 - val_accuracy: 0.9680 - lr: 1.2500e-04\n",
      "Epoch 15/25\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.1355 - accuracy: 0.9930\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 15: val_loss improved from 0.20774 to 0.20771, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.20774 to 0.20771, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 15: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 15: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.1358 - accuracy: 0.9930 - val_loss: 0.2077 - val_accuracy: 0.9687 - lr: 6.2500e-05\n",
      "Epoch 16/25\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.1355 - accuracy: 0.9932\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.20771\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.20771\n",
      "\n",
      "Epoch 16: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 16: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1355 - accuracy: 0.9931 - val_loss: 0.2078 - val_accuracy: 0.9681 - lr: 6.2500e-05\n",
      "Epoch 17/25\n",
      "40/44 [==========================>...] - ETA: 0s - loss: 0.1356 - accuracy: 0.9931\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 17: val_loss improved from 0.20771 to 0.20738, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.20771 to 0.20738, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 17: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 17: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1354 - accuracy: 0.9930 - val_loss: 0.2074 - val_accuracy: 0.9684 - lr: 6.2500e-05\n",
      "Epoch 18/25\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.1355 - accuracy: 0.9929\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.20738\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.20738\n",
      "\n",
      "Epoch 18: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 18: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1354 - accuracy: 0.9928 - val_loss: 0.2074 - val_accuracy: 0.9682 - lr: 6.2500e-05\n",
      "Epoch 19/25\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9930\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 19: val_loss improved from 0.20738 to 0.20679, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.20738 to 0.20679, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 19: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 19: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.1348 - accuracy: 0.9930 - val_loss: 0.2068 - val_accuracy: 0.9680 - lr: 3.1250e-05\n",
      "Epoch 20/25\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9931\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 20: val_loss improved from 0.20679 to 0.20662, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.20679 to 0.20662, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 20: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 20: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 20: saving model to jt_classification/KERAS_check_model_epoch20.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.1348 - accuracy: 0.9931 - val_loss: 0.2066 - val_accuracy: 0.9685 - lr: 3.1250e-05\n",
      "Epoch 21/25\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9931\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 21: val_loss improved from 0.20662 to 0.20648, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.20662 to 0.20648, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 21: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 21: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.1349 - accuracy: 0.9931 - val_loss: 0.2065 - val_accuracy: 0.9685 - lr: 3.1250e-05\n",
      "Epoch 22/25\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.1347 - accuracy: 0.9931\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.20648\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.20648\n",
      "\n",
      "Epoch 22: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 22: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.1348 - accuracy: 0.9932 - val_loss: 0.2071 - val_accuracy: 0.9679 - lr: 3.1250e-05\n",
      "Epoch 23/25\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.1339 - accuracy: 0.9933\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.20648\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.20648\n",
      "\n",
      "Epoch 23: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 23: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.1344 - accuracy: 0.9932 - val_loss: 0.2071 - val_accuracy: 0.9679 - lr: 1.5625e-05\n",
      "Epoch 24/25\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9930\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.20648\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.20648\n",
      "\n",
      "Epoch 24: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 24: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1344 - accuracy: 0.9930 - val_loss: 0.2074 - val_accuracy: 0.9687 - lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.1348 - accuracy: 0.9930\n",
      "***callbacks***\n",
      "saving losses to jt_classification/losses.log\n",
      "\n",
      "Epoch 25: val_loss improved from 0.20648 to 0.20635, saving model to jt_classification/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.20648 to 0.20635, saving model to jt_classification/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 25: saving model to jt_classification/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 25: saving model to jt_classification/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.1343 - accuracy: 0.9932 - val_loss: 0.2063 - val_accuracy: 0.9691 - lr: 1.5625e-05\n",
      "... quantizing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MNIST/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MNIST/assets\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "# from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "# from qkeras.utils import model_save_quantized_weights\n",
    "\n",
    "# pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0, begin_step=2000, frequency=100)}\n",
    "# model = prune.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# adam = Adam(lr=0.003)\n",
    "\n",
    "# model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "# callbacks= all_callbacks( outputDir = 'jt_classification')\n",
    "# callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "# model.fit(X_train, y_train, batch_size=1024,\n",
    "#           epochs=25,validation_split=0.25, verbose=1, shuffle=True,\n",
    "#           callbacks = callbacks.callbacks);\n",
    "# model = strip_pruning(model)\n",
    "# model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "# model_save_quantized_weights(model, \"test_weights\")\n",
    "# model.save(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 3ms/step - loss: 0.1901 - accuracy: 0.9739\n",
      " ACCURACY IS 0.9739000201225281\n"
     ]
    }
   ],
   "source": [
    "accuracy=model.evaluate(X_test,y_test)\n",
    "print(\" ACCURACY IS \"+str(accuracy[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        ...,\n",
       "        [ 0.      , -0.015625,  0.      , ...,  0.      ,  0.03125 ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         -0.015625]], dtype=float32),\n",
       " array([ 0.      , -0.0625  , -0.0625  ,  0.109375,  0.03125 ,  0.015625,\n",
       "         0.15625 ,  0.046875, -0.03125 , -0.046875, -0.09375 ,  0.109375,\n",
       "        -0.078125, -0.109375,  0.25    ,  0.109375,  0.      ,  0.125   ,\n",
       "         0.      , -0.03125 ,  0.109375,  0.015625,  0.015625, -0.171875,\n",
       "        -0.03125 , -0.015625, -0.09375 ,  0.      , -0.15625 ,  0.21875 ,\n",
       "        -0.015625,  0.0625  ,  0.171875,  0.09375 ,  0.      ,  0.015625,\n",
       "        -0.046875, -0.03125 ,  0.      ,  0.109375,  0.03125 ,  0.015625,\n",
       "        -0.0625  ,  0.15625 , -0.03125 ,  0.09375 ,  0.171875,  0.015625,\n",
       "         0.078125, -0.046875, -0.09375 ,  0.0625  ,  0.109375,  0.328125,\n",
       "         0.109375, -0.015625], dtype=float32),\n",
       " array([[ 0.      , -0.109375,  0.203125, ..., -0.234375,  0.      ,\n",
       "          0.015625],\n",
       "        [ 0.      ,  0.1875  ,  0.15625 , ..., -0.109375,  0.109375,\n",
       "          0.03125 ],\n",
       "        [ 0.21875 ,  0.109375,  0.03125 , ..., -0.34375 , -0.203125,\n",
       "          0.375   ],\n",
       "        ...,\n",
       "        [-0.15625 , -0.109375, -0.28125 , ...,  0.140625,  0.      ,\n",
       "         -0.03125 ],\n",
       "        [ 0.015625,  0.      ,  0.      , ...,  0.203125,  0.0625  ,\n",
       "          0.015625],\n",
       "        [ 0.234375,  0.      ,  0.      , ...,  0.21875 , -0.09375 ,\n",
       "          0.015625]], dtype=float32),\n",
       " array([-0.015625,  0.      ,  0.      ,  0.046875, -0.046875,  0.      ,\n",
       "         0.171875,  0.0625  , -0.0625  ,  0.109375,  0.015625,  0.0625  ,\n",
       "         0.015625,  0.140625,  0.0625  ,  0.046875,  0.0625  , -0.046875,\n",
       "         0.0625  ,  0.046875,  0.09375 ,  0.109375,  0.078125,  0.09375 ,\n",
       "        -0.015625,  0.046875,  0.03125 ,  0.0625  ,  0.015625,  0.078125,\n",
       "         0.09375 ,  0.09375 , -0.078125, -0.015625,  0.046875,  0.109375,\n",
       "        -0.078125, -0.03125 , -0.046875,  0.0625  ,  0.09375 ,  0.0625  ,\n",
       "         0.171875, -0.03125 , -0.0625  ,  0.      ,  0.03125 , -0.0625  ,\n",
       "         0.15625 ,  0.046875,  0.109375, -0.015625, -0.046875,  0.078125,\n",
       "         0.      ,  0.0625  ,  0.03125 ,  0.078125,  0.046875,  0.078125,\n",
       "         0.03125 ,  0.0625  ,  0.03125 ,  0.015625], dtype=float32),\n",
       " array([[-0.34375 , -0.03125 ,  0.      , ...,  0.21875 , -0.203125,\n",
       "          0.203125],\n",
       "        [ 0.      , -0.046875, -0.0625  , ...,  0.296875, -0.078125,\n",
       "         -0.03125 ],\n",
       "        [ 0.      , -0.15625 ,  0.28125 , ...,  0.21875 ,  0.      ,\n",
       "         -0.15625 ],\n",
       "        ...,\n",
       "        [ 0.109375, -0.125   , -0.203125, ..., -0.375   , -0.109375,\n",
       "         -0.203125],\n",
       "        [ 0.046875,  0.171875,  0.      , ...,  0.      ,  0.328125,\n",
       "         -0.0625  ],\n",
       "        [ 0.      ,  0.      ,  0.453125, ...,  0.0625  , -0.015625,\n",
       "          0.25    ]], dtype=float32),\n",
       " array([ 0.046875,  0.015625,  0.125   ,  0.109375,  0.09375 ,  0.      ,\n",
       "         0.03125 ,  0.09375 ,  0.0625  , -0.03125 ,  0.046875,  0.      ,\n",
       "         0.03125 ,  0.015625,  0.015625, -0.03125 ,  0.046875,  0.078125,\n",
       "         0.046875,  0.078125,  0.109375,  0.09375 ,  0.09375 ,  0.      ,\n",
       "         0.140625,  0.03125 ,  0.078125,  0.09375 ,  0.046875,  0.03125 ,\n",
       "         0.015625,  0.046875], dtype=float32),\n",
       " array([[ 0.3125  , -0.25    ,  0.4375  , -0.40625 ,  0.53125 , -0.640625,\n",
       "          0.1875  , -0.390625, -0.25    , -0.03125 ],\n",
       "        [-0.25    ,  0.5     ,  0.359375,  0.40625 , -0.390625, -0.203125,\n",
       "          0.34375 , -0.296875, -0.390625, -0.171875],\n",
       "        [-0.4375  , -0.84375 ,  0.453125,  0.015625,  0.40625 , -0.515625,\n",
       "         -0.390625,  0.484375,  0.3125  ,  0.15625 ],\n",
       "        [ 0.      ,  0.265625, -0.09375 , -0.515625, -0.265625,  0.      ,\n",
       "         -0.171875,  0.296875,  0.578125, -0.703125],\n",
       "        [-0.5     ,  0.21875 ,  0.046875, -0.40625 ,  0.25    , -0.46875 ,\n",
       "          0.390625, -0.515625,  0.359375,  0.046875],\n",
       "        [ 0.640625, -0.109375, -0.265625, -0.25    , -0.421875, -0.265625,\n",
       "         -0.546875,  0.328125, -0.28125 ,  0.484375],\n",
       "        [ 0.640625,  0.5     ,  0.203125,  0.34375 , -0.203125,  0.0625  ,\n",
       "         -0.53125 , -0.390625, -0.5     , -0.03125 ],\n",
       "        [ 0.4375  , -0.578125, -0.078125, -0.625   , -0.1875  ,  0.234375,\n",
       "          0.296875, -0.640625,  0.328125,  0.421875],\n",
       "        [-0.1875  ,  0.4375  ,  0.296875,  0.453125, -0.015625, -0.65625 ,\n",
       "         -0.296875, -0.03125 , -0.234375, -0.28125 ],\n",
       "        [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "          0.      ,  0.      ,  0.      ,  0.      ],\n",
       "        [ 0.53125 ,  0.25    , -0.421875, -0.421875,  0.46875 , -0.234375,\n",
       "         -0.34375 ,  0.53125 , -0.25    , -0.140625],\n",
       "        [ 0.1875  ,  0.21875 ,  0.375   , -0.59375 , -0.578125,  0.5     ,\n",
       "          0.1875  ,  0.25    , -0.609375, -0.484375],\n",
       "        [ 0.25    , -0.703125,  0.1875  , -0.609375, -0.109375, -0.78125 ,\n",
       "          0.40625 ,  0.46875 , -0.5625  ,  0.28125 ],\n",
       "        [ 0.390625, -0.515625,  0.      ,  0.      , -0.578125,  0.15625 ,\n",
       "          0.328125,  0.4375  , -0.046875,  0.515625],\n",
       "        [-0.03125 , -0.421875, -0.40625 ,  0.25    , -0.625   ,  0.453125,\n",
       "          0.53125 , -0.3125  ,  0.34375 , -0.171875],\n",
       "        [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "          0.      ,  0.      ,  0.      ,  0.      ],\n",
       "        [ 0.390625,  0.03125 , -0.515625,  0.      , -0.015625, -0.390625,\n",
       "          0.625   , -0.1875  , -0.15625 , -0.34375 ],\n",
       "        [ 0.3125  ,  0.421875, -0.640625, -0.453125,  0.609375,  0.515625,\n",
       "         -0.0625  , -0.5     ,  0.      , -0.0625  ],\n",
       "        [-0.375   , -0.296875,  0.375   , -0.53125 ,  0.25    , -0.390625,\n",
       "          0.484375, -0.453125, -0.203125, -0.4375  ],\n",
       "        [-0.34375 , -0.21875 , -0.328125,  0.46875 ,  0.5     ,  0.4375  ,\n",
       "         -0.265625, -0.421875,  0.40625 ,  0.484375],\n",
       "        [-0.390625, -0.484375,  0.15625 ,  0.078125,  0.359375, -0.53125 ,\n",
       "         -0.640625, -0.4375  ,  0.375   ,  0.3125  ],\n",
       "        [ 0.546875, -0.0625  ,  0.46875 ,  0.421875, -0.296875, -0.109375,\n",
       "         -0.46875 , -0.421875,  0.328125, -0.578125],\n",
       "        [-0.4375  , -0.453125, -0.515625,  0.265625, -0.265625,  0.46875 ,\n",
       "         -0.546875,  0.4375  ,  0.234375, -0.21875 ],\n",
       "        [-0.46875 ,  0.359375, -0.578125,  0.3125  , -0.3125  , -0.3125  ,\n",
       "         -0.40625 ,  0.140625, -0.140625,  0.546875],\n",
       "        [-0.296875,  0.5     ,  0.5     ,  0.4375  , -0.390625, -0.65625 ,\n",
       "         -0.15625 ,  0.15625 ,  0.59375 , -0.78125 ],\n",
       "        [-0.453125,  0.28125 , -0.09375 ,  0.234375,  0.46875 ,  0.34375 ,\n",
       "         -0.734375,  0.4375  , -0.828125,  0.234375],\n",
       "        [-0.515625,  0.28125 , -0.0625  ,  0.078125, -0.5625  ,  0.140625,\n",
       "         -0.40625 , -0.71875 ,  0.296875,  0.1875  ],\n",
       "        [-0.34375 ,  0.34375 , -0.546875, -0.03125 , -0.21875 , -0.609375,\n",
       "         -0.203125,  0.34375 ,  0.359375,  0.546875],\n",
       "        [-0.359375, -0.234375,  0.546875,  0.59375 , -0.140625, -0.078125,\n",
       "         -0.484375,  0.21875 , -0.390625, -0.390625],\n",
       "        [-0.53125 , -0.328125, -0.53125 ,  0.296875,  0.546875,  0.59375 ,\n",
       "         -0.015625,  0.546875, -0.578125, -0.53125 ],\n",
       "        [-0.5     , -0.4375  , -0.609375,  0.296875, -0.25    ,  0.015625,\n",
       "          0.40625 , -0.53125 , -0.21875 ,  0.453125],\n",
       "        [-0.53125 ,  0.      ,  0.21875 , -0.53125 , -0.65625 ,  0.453125,\n",
       "          0.453125, -0.1875  ,  0.046875, -0.625   ]], dtype=float32),\n",
       " array([ 0.09375 ,  0.03125 , -0.125   , -0.09375 ,  0.09375 ,  0.140625,\n",
       "         0.015625, -0.015625,  0.      , -0.09375 ], dtype=float32)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import estimate as es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'estimate' from '/home/edge/Desktop/argykokk/hls4ml-tutorial/networks/MNIST/estimate.py'>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=2\n",
    "reuse=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero weight are:  5100\n",
      "non-zero weight are:  5652\n",
      "Mul ins = 120 and Max muls = 57 and Saved muls = 63 Reuse factor = 100\n",
      "DSPs: 57\n",
      "LUT cost2= 22121 bias acc= 784 mult acc78344\n",
      "LUTs prediction: 101249\n",
      "Initial muls: 10752 Real muls: 5652 Initial neurons: 56 Tuned neurons: 30\n",
      "FFs prediction: 107859\n",
      "zero weight are:  936\n",
      "non-zero weight are:  2648\n",
      "Mul ins = 219 and Max muls = 27 and Saved muls = 192 Reuse factor = 100\n",
      "DSPs: 27\n",
      "LUT cost2= 13875 bias acc= 896 mult acc36176\n",
      "LUTs prediction: 50947\n",
      "Initial muls: 3584 Real muls: 2648 Initial neurons: 64 Tuned neurons: 48\n",
      "FFs prediction: 49563\n",
      "zero weight are:  576\n",
      "non-zero weight are:  1472\n",
      "Mul ins = 188 and Max muls = 15 and Saved muls = 173 Reuse factor = 100\n",
      "DSPs: 15\n",
      "LUT cost2= 10863 bias acc= 448 mult acc20160\n",
      "LUTs prediction: 31471\n",
      "Initial muls: 2048 Real muls: 1472 Initial neurons: 32 Tuned neurons: 23\n",
      "FFs prediction: 28355\n",
      "zero weight are:  27\n",
      "non-zero weight are:  293\n",
      "Mul ins = 120 and Max muls = 3 and Saved muls = 117 Reuse factor = 100\n",
      "DSPs: 3\n",
      "LUT cost2= 2289 bias acc= 140 mult acc3962\n",
      "LUTs prediction: 6391\n",
      "Initial muls: 320 Real muls: 293 Initial neurons: 10 Tuned neurons: 10\n",
      "FFs prediction: 6910\n",
      "CPU times: user 92.3 ms, sys: 7.93 ms, total: 100 ms\n",
      "Wall time: 98.4 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/edge/miniconda3/envs/hls4ml-tutorial/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#input_num, neurons_num, layer_id, model\n",
    "ffs = es.estimate(192,56,0,model,reuse,param)\n",
    "luts, ffs = es.estimate(56,64,2,model,reuse,param)\n",
    "luts, ffs = es.estimate(64,32,4,model,reuse,param)\n",
    "luts, ffs = es.estimate(32,10,6,model,reuse,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: fc1_input, layer type: InputLayer, input shapes: [[None, 196]], output shape: [None, 196]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 196]], output shape: [None, 56]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 56]], output shape: [None, 56]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 56]], output shape: [None, 64]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 64]], output shape: [None, 64]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 64]], output shape: [None, 32]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 10]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "-----------------------------------\n",
      "Model\n",
      "  Precision:         fixed<16,6>\n",
      "  ReuseFactor:       1\n",
      "  Strategy:          Latency\n",
      "  BramFactor:        1000000000\n",
      "  TraceOutput:       False\n",
      "LayerName\n",
      "  fc1_input\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  fc1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,2>\n",
      "      bias:          fixed<8,2>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  fc1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  relu1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<8,1,RND_CONV,SAT>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  fc2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,2>\n",
      "      bias:          fixed<8,2>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  fc2_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  relu2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<8,1,RND_CONV,SAT>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  fc3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,2>\n",
      "      bias:          fixed<8,2>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  fc3_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  relu3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<8,1,RND_CONV,SAT>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  output\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,2>\n",
      "      bias:          fixed<8,2>\n",
      "      accum:         fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "  output_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "  softmax\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      table:         fixed<18,8>\n",
      "      exp_table:     fixed<18,8,RND,SAT>\n",
      "      inv_table:     fixed<18,8,RND,SAT>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Implementation:  stable\n",
      "    Skip:            False\n",
      "    exp_table_t:     ap_fixed<18,8>\n",
      "    inv_table_t:     ap_fixed<18,4>\n",
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: fc1_input, layer type: InputLayer, input shapes: [[None, 196]], output shape: [None, 196]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 196]], output shape: [None, 56]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 56]], output shape: [None, 56]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 56]], output shape: [None, 64]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 64]], output shape: [None, 64]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 64]], output shape: [None, 32]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 32]], output shape: [None, 32]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 32]], output shape: [None, 10]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='mnist/hls4ml_prj', part='xc7z007s-clg225-2'\n",
    ")\n",
    "hls_model.compile()\n",
    "\n",
    "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
